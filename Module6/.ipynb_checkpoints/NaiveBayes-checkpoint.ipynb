{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Models\n",
    "\n",
    "In this lab you will work with **naive Bayes models**. Naive Bayes models are a surprisingly useful and effective simplification of the general Bayesian models. Naive Bayes models make the naive assumption of statistical independence of the features. In many cases, naive Bayes module are surprisingly effective despite violating the assumption of independence. \n",
    "\n",
    "In simple terms, naive Bayes models use empirical distributions of the features to compute probabilities of the labels. The naive Bayes models can use most any family of distributions for the features. It is important to select the correct distribution family for the data you are working with. Common cases are:\n",
    "- **Gaussian;** for conditions or numerical features.\n",
    "- **Bernoulli;** for features with binary values. \n",
    "- **Multinomial;** for features with more than two categories. \n",
    "\n",
    "These is one pit fall, the model fails if a zero probability is encountered. This situation occurs when there is a 'hole' in the sample space where there are no samples. A simple smoothing procedure can deal with this problem. The smoothing hyperparameter, usually called alpha, is one of the few required for naive Bayes models. \n",
    "\n",
    "Some properties of naive Bayes models are:\n",
    "- Computational complexity is linear in number of parameter/features, making naive Bayes models highly scalable. There are out or core approaches suitable for massive datasets.\n",
    "- Requires minimal data to produce models that generalizes well. If there are only a few cases per category to train a model a naive Bayes model can be a good choice. \n",
    "- Have a simple and inherent regularization.\n",
    "\n",
    "Naive Bayes models are used in many situations including:\n",
    "\n",
    "- Document classification\n",
    "- SPAM detection\n",
    "- Image classification \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iris dataset\n",
    "\n",
    "As a first example you will use a naive Bayes model to classify the species of iris flowers. \n",
    "\n",
    "As a first step, execute the code in the cell below to load the required packages to run the rest of this notebook. \n",
    "\n",
    "> **Note:** If you are running in Azure Notebooks, make sure that you run the code in the `setup.ipynb` notebook at the start of you session to ensure your environment is correctly configured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following object is masked from 'package:gridExtra':\n",
      "\n",
      "    combine\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Attaching package: 'MLmetrics'\n",
      "\n",
      "The following objects are masked from 'package:caret':\n",
      "\n",
      "    MAE, RMSE\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    Recall\n",
      "\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in library(klaR): there is no package called 'klaR'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(klaR): there is no package called 'klaR'\nTraceback:\n",
      "1. library(klaR)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "## Import packages\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(repr)\n",
    "library(dplyr)\n",
    "library(caret)\n",
    "library(e1071)\n",
    "library(MLmetrics)\n",
    "library(klaR)\n",
    "\n",
    "options(repr.plot.width=4, repr.plot.height=4) # Set the initial plot area dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for these data, you will now load and plot them. Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_plot = function(df, colx, coly){\n",
    "    ggplot(df, aes_string(colx,coly)) +\n",
    "          geom_point(aes(color = factor(df$Species)), alpha = 0.4)\n",
    "}\n",
    "\n",
    "plot_iris = function(df){\n",
    "    options(repr.plot.width=8, repr.plot.height=5)\n",
    "    grid.arrange(\n",
    "        single_plot(df, 'Sepal.Length', 'Sepal.Width'),\n",
    "        single_plot(df, 'Sepal.Length', 'Petal.Length'),\n",
    "        single_plot(df, 'Petal.Length', 'Petal.Width'),\n",
    "        single_plot(df, 'Sepal.Width', 'Petal.Length'),\n",
    "        nrow = 2)\n",
    "}\n",
    "\n",
    "head(iris, 10)   \n",
    "plot_iris(iris)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Setosa (in red) is well separated from the other two categories. The Versicolor (in green) and the Virginica (in blue) show considerable overlap. The question is how well our classifier will separate these categories. \n",
    "\n",
    "Next, execute the code in the cell below to split the dataset into test and training set. Notice that unusually, 67% of the cases are being used as the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1955)\n",
    "## Randomly sample cases to create independent training and test data\n",
    "partition = createDataPartition(iris[,'Species'], times = 1, p = 0.33, list = FALSE)\n",
    "training = iris[partition,] # Create the training sample\n",
    "dim(training)\n",
    "test = iris[-partition,] # Create the test sample\n",
    "dim(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is always the case with machine learning, numeric features  must be scaled. Execute the code in the cell below to scale the training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width')\n",
    "preProcValues <- preProcess(training[,num_cols], method = c(\"center\", \"scale\"))\n",
    "\n",
    "training[,num_cols] = predict(preProcValues, training[,num_cols])\n",
    "test[,num_cols] = predict(preProcValues, test[,num_cols])\n",
    "head(training[,num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will define and fit a naive Bayes model. The code in the cell uses the `naiveBayes` function from the e1071 package. The model formula is specified along with the data. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_mod = naiveBayes(factor(Species) ~ ., data = training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the code in the cell below uses the `predict` method is used to compute the multinomial class probabilities from the scaled features. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test[,'scores'] = predict(nb_mod, newdata = test)\n",
    "test[1:10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see actual species and predicted species. \n",
    "\n",
    "It is time to evaluate the model results. Keep in mind that the problem has been made deliberately difficult, by having more test cases than training cases. The iris data has three species categories. Therefore it is necessary to use evaluation code for a three category problem. The function in the cell below extends code from previous labs to deal with a three category problem. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics = function(df, label){\n",
    "    ## Compute and print the confusion matrix\n",
    "    cm = as.matrix(table(Actual = df$Species, Predicted = df$scores))\n",
    "    print(cm)\n",
    "\n",
    "    ## Compute and print accuracy \n",
    "    accuracy = round(sum(sapply(1:nrow(cm), function(i) cm[i,i]))/sum(cm), 3)\n",
    "    cat('\\n')\n",
    "    cat(paste('Accuracy = ', as.character(accuracy)), '\\n \\n')                           \n",
    "\n",
    "    ## Compute and print precision, recall and F1\n",
    "    precision = sapply(1:nrow(cm), function(i) cm[i,i]/sum(cm[i,]))\n",
    "    recall = sapply(1:nrow(cm), function(i) cm[i,i]/sum(cm[,i]))    \n",
    "    F1 = sapply(1:nrow(cm), function(i) 2*(recall[i] * precision[i])/(recall[i] + precision[i]))    \n",
    "    metrics = sapply(c(precision, recall, F1), round, 3)        \n",
    "    metrics = t(matrix(metrics, nrow = nrow(cm), ncol = ncol(cm)))       \n",
    "    dimnames(metrics) = list(c('Precision', 'Recall', 'F1'), unique(test$Species))      \n",
    "    print(metrics)\n",
    "}  \n",
    "print_metrics(test, 'Species')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice the following:\n",
    "1. The confusion matrix has dimension 3X3. You can see that most cases are correctly classified. \n",
    "2. The overall accuracy is 0.92. Since the classes are roughly balanced, this metric indicates relatively good performance of the classifier, particularly since it was only trained on 51 cases. \n",
    "3. The precision, recall and  F1 for each of the classes is relatively good. Virginica has the worst metrics since it has the largest number of misclassified cases. \n",
    "\n",
    "How important are each of the features for this model? The R Caret package provides the capability to find out. As a first step, `gbm` models must be trained using the the Caret `train` function. The code in the cell below does this, using the default model arguments. The default arguments for the model are specified with the `tuneGrid` argument of `train`. Execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trControl <- trainControl(method = \"cv\", number = 10)\n",
    "\n",
    "nb_mod_train = train(factor(Species) ~ ., \n",
    "                      data = training, \n",
    "                      method = \"nb\", \n",
    "                      trControl = trControl)\n",
    "nb_mod_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Caret model object trained, the feature importance can be computed and displayed. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "imp = varImp(nb_mod_train, scale = FALSE)$importance\n",
    "imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the table above. Notice that `Sepal.Width` has the least importance for classifying each of the species. \n",
    "\n",
    "Execute this code, and answer **Question 1** on the course page.\n",
    "\n",
    "Next, you will train and evaluate a model using the three most important features by executing the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_mod_3 = naiveBayes(factor(Species) ~ Sepal.Length + Petal.Length + Petal.Width, data = training)\n",
    "test[,'scores'] = predict(nb_mod_3, newdata = test)\n",
    "print_metrics(test, 'Species')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are slightly better than the for the model with the complete feature set. Naive Bayes models work well with a minimal number of features. Further, the simpler model is preferred since it is likely to generalize better. \n",
    "\n",
    "The code in the cell below plots the classes of the iris flower along with the classification errors shown by shape. Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create column of correct-incorrect classification\n",
    "test$correct = ifelse(test$Species == test$scores, 'correct', 'incorrect')\n",
    "\n",
    "single_plot_classes = function(df, colx, coly){\n",
    "    ggplot(df, aes_string(colx,coly)) +\n",
    "          geom_point(aes(color = factor(df$Species), shape = correct), alpha = 0.4)\n",
    "}\n",
    "\n",
    "plot_iris_classes = function(df){\n",
    "    options(repr.plot.width=8, repr.plot.height=5)\n",
    "    grid.arrange(\n",
    "        single_plot_classes(df, 'Sepal.Length', 'Sepal.Width'),\n",
    "        single_plot_classes(df, 'Sepal.Length', 'Petal.Length'),\n",
    "        single_plot_classes(df, 'Petal.Length', 'Petal.Width'),\n",
    "        single_plot_classes(df, 'Sepal.Width', 'Petal.Length'),\n",
    "        nrow = 2)\n",
    "}\n",
    "\n",
    "plot_iris_classes(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these plots. You can see how the classifier has divided the feature space between the classes. Notice that most of the errors occur in the overlap region between Virginica and Versicolor. This behavior is to be expected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "Now, you will try a more complex example using the credit scoring data. You will use the prepared data which has been prepared by removing duplicate cases. Some columns which are know not to be predictive are removed. Execute the code in the cell below to load the dataset for the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credit = read.csv('German_Credit_Preped.csv', header = TRUE)\n",
    "## Subset the data frame\n",
    "credit = credit[,c('checking_account_status', 'loan_duration_mo', 'credit_history', 'loan_amount', 'savings_account_balance',\n",
    "                   'time_employed_yrs', 'payment_pcnt_income', 'time_in_residence', 'property', 'age_yrs',\n",
    "                   'other_credit_outstanding', 'number_loans', 'job_category', 'dependents', 'telephone', 'bad_credit' )]\n",
    "print(dim(credit))\n",
    "names(credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation will be used to train the model. Since folds will be selected from the entire dataset the numeric features are scaled in batch. Execute the code in the cell below to accomplish this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = c('loan_duration_mo', 'loan_amount', 'payment_pcnt_income',\n",
    "             'time_in_residence', 'age_yrs', 'number_loans', 'dependents')\n",
    "\n",
    "preProcValues <- preProcess(credit[,num_cols], method = c(\"center\", \"scale\"))\n",
    "credit[,num_cols] = predict(preProcValues, credit[,num_cols])\n",
    "head(credit[,num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R Caret package computes most performance metrics using the positive cases. For example, recall is a measure of correct classification of positive cases. Therefore, it is important to have the coding of the label correct. The code in the cell below creates a factor (categorical) variable and coerces the levels of the label column, `bad_credit`. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit$bad_credit <- ifelse(credit$bad_credit == 1, 'bad', 'good')\n",
    "credit$bad_credit <- factor(credit$bad_credit, levels = c('bad', 'good'))\n",
    "credit$bad_credit[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the results above you can see the new coding of the label column along with the levels, {'bad', 'good'}. \n",
    "\n",
    "As the inner loop of a nested cross validation, the code in the cell below uses the capability of the R Caret package to estimate the best hyperparameters using 5 fold cross validation. This first cross validation is performed using ROC as the metric. There are a few points to note here:\n",
    "1. A Caret `trainControl` object is used to define the 5 fold cross validation. The `twoClassSummary` function is specified, making ROC the metric for hyperparameter optimization. The `nb` model does not accommodate case weights. However, in the case **up-sampling** or **over-sampling** is used. Up-sampling randomly samples the minority case so that the number of classes in each training fold is balanced. \n",
    "2. The model is trained using all features as can be seen from the model formula in the Caret `train` function. \n",
    "3. `ROC` is specified as a `metric` in the call to `train`. \n",
    "4. Weights are specified to help with the class imbalance and the cost imbalance of misclassification of bad credit customers. \n",
    "5. The `train` function uses a `tuneGrid` argument to define the hyperparameters to search. \n",
    "\n",
    "Execute this code, examine the result, and answer **Question 3** on the course page.\n",
    "\n",
    "****\n",
    "**Note:** Naive Bayes models work well with a limited amount of data. However, if there are categorical features with rare categories there will likely be problems with zero probabilities. This condition occurs whenever there are no samples for dummy variable within the sample space of the other features. When this situation occurs, the R naive Bayes model software will generate warning messages, and the algorithm may not converge at all.  \n",
    "\n",
    "The solution is to use **Laplace smoothing**. This method smooths the conditional probability distribution estimated in the naive Bayes model preventing zero probabilities from occurring. In the naive Bayes software this requires specifying a Laplace smoothing parameter, `fL`, a kernel, `useKernel = TRUE`, and the kernel bandwidth adjustment, `adjust`. Combinations of `fL` and `adjust` must be selected which prevent zero probabilities from occurring.   \n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fitControl <- trainControl(method = \"cv\",\n",
    "                           number = 5,\n",
    "                           sampling = 'up',\n",
    "                           returnResamp=\"all\",\n",
    "                           savePredictions = TRUE,\n",
    "                           classProbs = TRUE,\n",
    "                           \n",
    "                           summaryFunction = twoClassSummary)\n",
    "paramGrid <- expand.grid(fL = c(0.01, 0.05, 0.1), usekernel = c(TRUE), adjust = c(0.5, 1.0))\n",
    "\n",
    "set.seed(1234)\n",
    "nb_fit_inside_tw <- train(bad_credit ~ ., \n",
    "                          data = credit,  \n",
    "                          method = \"nb\", # naive Bayes model \n",
    "                          trControl = fitControl,\n",
    "                          tuneGrid = paramGrid,\n",
    "                          metric=\"ROC\")\n",
    "print(nb_fit_inside_tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid of hyperpameters searched by the Caret package is over Laplace smoother parameter `fL` and the kernel span parameter `adjust`. The grid along with the ROC and other metrics is shown in the printed table. The ROC and sensitivity (global recall) metrics are good. However, the specificity is rather poor. \n",
    "\n",
    "The hyperparameter optimization can also be performed using Recall as a metric. The code in the cell below uses the `prSummary` function for the `summaryFunction` argument for `trainControl` and sets the `metric` as `Recall`. Execute this call and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitControl <- trainControl(method = \"cv\",\n",
    "                           number = 5,\n",
    "                           sampling = 'up',\n",
    "                           returnResamp=\"all\",\n",
    "                           savePredictions = TRUE,\n",
    "                           classProbs = TRUE,\n",
    "                           summaryFunction = prSummary)\n",
    "paramGrid <- expand.grid(fL = c(0.01, 0.05, 0.1), usekernel = c(TRUE), adjust = c(0.5, 1.0))\n",
    "\n",
    "set.seed(1234)\n",
    "nb_fit_inside_pr <- train(bad_credit ~ ., \n",
    "                          data = credit,  \n",
    "                          method = \"nb\", # naive Bayes model \n",
    "                          trControl = fitControl,\n",
    "                          tuneGrid = paramGrid,\n",
    "                          metric=\"Recall\")\n",
    "print(nb_fit_inside_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the optimal hyperparameters for the model trained with ROC, which features are the most important? The code in the cell below computes and displays feature importance using the Caret `varImp` function. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=6)\n",
    "var_imp = varImp(nb_fit_inside_tw)\n",
    "print(var_imp)\n",
    "plot(var_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that at least two of the features should be removed from the dataset. Execute the code in the cell below to select the required features and label columns.\n",
    "\n",
    "****\n",
    "**Note:** Naive Bayes use estimates of conditional probability and are there less susceptible to over-fitting when compared to most other machine learning models. None the less, removing unimportant features can improve generalization since these features can contain unexpected values or noise.    \n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_reduced =  credit[,c('checking_account_status', 'loan_duration_mo', 'credit_history', 'loan_amount', 'savings_account_balance',\n",
    "                   'time_employed_yrs', 'payment_pcnt_income', 'property', 'age_yrs',\n",
    "                   'other_credit_outstanding', 'number_loans', 'job_category', 'telephone', 'bad_credit' )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to perform the cross validation grid search using ROC as the metric with the reduced feature set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitControl <- trainControl(method = \"cv\",\n",
    "                           number = 5,\n",
    "                           sampling = 'up',\n",
    "                           returnResamp=\"all\",\n",
    "                           savePredictions = TRUE,\n",
    "                           classProbs = TRUE,\n",
    "                           summaryFunction = twoClassSummary)\n",
    "paramGrid <- expand.grid(fL = c(0.01, 0.05, 0.1), usekernel = c(TRUE), adjust = c(0.5, 1.0))\n",
    "\n",
    "set.seed(1234)\n",
    "nb_fit_inside_tw <- train(bad_credit ~ ., \n",
    "                          data = credit_reduced,  \n",
    "                          method = \"nb\", # naive Bayes model \n",
    "                          trControl = fitControl,\n",
    "                          tuneGrid = paramGrid,\n",
    "                          metric=\"ROC\")\n",
    "print(nb_fit_inside_tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the cross validation grid search with the reduced feature set are nearly the same as the first result. The ROC and sensitivity (global recall) metrics are good. However, the specificity is rather poor. Evidentially, pruning these features was the correct step. This process can be continued, but will not be in this lab in the interest of reducing length. \n",
    "\n",
    "To better understand the parameter sweep, execute the code in the cell below to create a chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=5, repr.plot.height=4)\n",
    "trellis.par.set(caretTheme())\n",
    "plot(nb_fit_inside_tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of ROC values is rather small. However, the plots shows that both bandwidth adjustment and Laplace correction have a noticeable effect on the model. \n",
    "\n",
    "Finally, to verify that the model will generalize well it is time to perform the outside CV loop. The code in the cell below defines a parameter grid with just the optimal hyperparameter values and using ROC as the metric. The CV then repeatedly fits the model with these hyperparameter values. Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid <- expand.grid(fL = c(0.05), usekernel = c(TRUE), adjust = c(1.0))\n",
    "\n",
    "set.seed(1234)\n",
    "nb_fit_outside_tw <- train(bad_credit ~ ., \n",
    "                          data = credit_reduced,  \n",
    "                          method = \"nb\", # naive Bayes model \n",
    "                          trControl = fitControl,\n",
    "                          tuneGrid = paramGrid,\n",
    "                          metric=\"ROC\")\n",
    "\n",
    "print_metrics = function(mod){\n",
    "    means = c(apply(mod$resample[,1:3], 2, mean), fL = mod$resample[1,4], usekernel = mod$resample[1,5], \n",
    "              adjust = mod$resample[1,6],Resample = 'Mean')\n",
    "    stds = c(apply(mod$resample[,1:3], 2, sd), fL = mod$resample[1,4], usekernel = mod$resample[1,5], \n",
    "              adjust = mod$resample[1,6],Resample = 'STD')\n",
    "    out = rbind(mod$resample, means, stds)\n",
    "    out[,1:3] = lapply(out[,1:3], function(x) round(as.numeric(x), 3))\n",
    "    out\n",
    "}\n",
    "print_metrics(nb_fit_outside_tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. As before, ROC and sensitivity are good, whereas specificity is poor. Notice that the standard deviation of the mean of the AUC are approximately an order of magnitude smaller than the mean. This indicates that this model is likely to generalize well. \n",
    "\n",
    "***\n",
    "**Note:** The predict method can be used with this optimal model to classify unknown cases.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have accomplished the following:\n",
    "1. Used a naive Bayes classifier for the iris dataset. The classification of the multiple species was reasonably effective. \n",
    "2. Fit a Gaussian naive Bayes model on the bank credit data. This model required hyperparameter choices that ensured sufficient Laplace smoothing to prevent zero conditional probabilities from occurring.\n",
    "3. To deal with the label class imbalance up-sampling was used. The specificity metric was poor regardless. \n",
    "4. Applied feature importance was used for feature selection. The model created and evaluated with the reduced feature set had essentially the same performance as the model with more features. \n",
    "5. Used the outer loop of the nested cross validation to demonstrate that the model is likely to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
