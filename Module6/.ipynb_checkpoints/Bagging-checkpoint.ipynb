{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Random Forest Models\n",
    "\n",
    "Using **ensemble methods** can greatly improve the results achieved with weak machine learning algorithms, also called **weak learners**. Ensemble methods achieve better performance by aggregating the results of many statistically independent models. This process averages out the errors and produces a final better prediction. \n",
    "\n",
    "In this lab you will work with a widely used ensemble method known as **bootstrap aggregating** or simply **bagging**. Bagging follows a simple procedure:\n",
    "1. N learners (machine learning models) are defined. \n",
    "2. N subsamples of the training data are created by **Bernoulli sampling with replacement**.\n",
    "3. The N learners are trained on the subsamples of the training data.\n",
    "4. The ensemble is scored by averaging, or taking a majority vote, of the predictions from the N learners.\n",
    "\n",
    "**Classification and regression tree models** are most typically used with bagging methods. The most common such algorithm is know as the **random forest**. The random forest method is highly scalable and generally produces good results, even for complex problems. \n",
    "\n",
    "Classification and regression trees tend to be robust to noise or outliers in the training data. This is true for the random forest algorithm as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iris dataset\n",
    "\n",
    "As a first example you will use random forest to classify the species of iris flowers. \n",
    "\n",
    "As a first step, execute the code in the cell below to load the required packages to run the rest of this notebook. \n",
    "\n",
    "> **Note:** If you are running in Azure Notebooks, make sure that you run the code in the `setup.ipynb` notebook at the start of you session to ensure your environment is correctly configured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Import packages ggplot2, gridExtra, repr, dplyr, caret, randomForest, MLmetrics\n",
    "\n",
    "\n",
    "# Set the initial plot area dimensions\n",
    "options(repr.plot.width=4, repr.plot.height=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below displays the head of the data frame and plots all pairwise combinations of the features with the species of the iris flower in colors. Use and finish this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_plot = function(df, colx, coly){\n",
    "    ggplot(df, aes_string(colx,coly)) +\n",
    "          geom_point(aes(color = factor(df$Species)), alpha = 0.4)\n",
    "}\n",
    "\n",
    "plot_iris = function(df){\n",
    "    options(repr.plot.width=8, repr.plot.height=5)\n",
    "    grid.arrange(\n",
    "        single_plot(df, 'Sepal.Length', 'Sepal.Width'),\n",
    "        single_plot(df, 'Sepal.Length', 'Petal.Length'),\n",
    "        single_plot(df, 'Petal.Length', 'Petal.Width'),\n",
    "        single_plot(df, 'Sepal.Width', 'Petal.Length'),\n",
    "        nrow = 2)\n",
    "}\n",
    "\n",
    "# inspect the iris dataset, this data is already available\n",
    "\n",
    "# now plot the iris data with the plot_iris() function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Setosa (in blue) is well separated from the other two categories. The Versicolor (in orange) and the Virginica (in green) show considerable overlap. The question is how well our classifier will separate these categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, finish the code in the cell below to split the dataset into test and training set. Notice that unusually, 67% of the cases are being used as the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"createDataPartition\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"createDataPartition\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# set the seed to 1955\n",
    "\n",
    "## Randomly sample cases to create independent training (33% of the data) and test data (67% of the data)\n",
    "partition = createDataPartition(iris[,'Species'], times = 1, p = 0.33, list = FALSE)\n",
    "\n",
    "# Create the training sample \"training\" based on prior partition\n",
    "\n",
    "# inspect the dimension of \"training\"\n",
    "\n",
    "# Create the test sample \"test\" based on prior partition\n",
    "\n",
    "# inspect the dimension of \"test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is always the case with machine learning, numeric features  must be scaled. Finish the code in the cell below to scale the training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the variable \"num_cols\" and assign 'Sepal.Length', 'Sepal.Width', 'Petal.Length', and 'Petal.Width' to it\n",
    "\n",
    "# pre process the numeric variables\n",
    "preProcValues <- preProcess(training[,num_cols], method = c(\"center\", \"scale\"))\n",
    "\n",
    "# use the predict function and the preProcValues to apply this transformation to the numeric columns of the \n",
    "# training and test data\n",
    "\n",
    "\n",
    "# inspect the numeric columns of the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will define and fit a random forest model. The code in the cell below will define a random forest model with 5 trees using the `randomForest` function from the R randomForest package, and then fits the model. Create this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed to 1115\n",
    "\n",
    "# create a random forest model \"rf_mod\" by training a model with randomForest() to predict \"Species\" of the training data.\n",
    "# use all the available remaining variables as predictors\n",
    "# use ?randomForest in R studio to inspect this function\n",
    "# set the number of trees (ntrees) to 5\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, the code in the cell below will use the `predict` method to compute the class score from the scaled features. Create this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the rf_mod to predict the test data with, using predict(), and using test as newdata. Assign this prediction to test$scores\n",
    "\n",
    "# inspect the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to evaluate the model results. Keep in mind that the problem has been made difficult deliberately, by having more test cases than training cases. \n",
    "\n",
    "The iris data has three species categories. Therefore it is necessary to use evaluation code for a three category problem. The function in the cell below extends code from previous labs to deal with a three category problem. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics = function(df, label){\n",
    "    ## Compute and print the confusion matrix\n",
    "    cm = as.matrix(table(Actual = df$Species, Predicted = df$scores))\n",
    "    print(cm)\n",
    "\n",
    "    ## Compute and print accuracy \n",
    "    accuracy = round(sum(sapply(1:nrow(cm), function(i) cm[i,i]))/sum(cm), 3)\n",
    "    cat('\\n')\n",
    "    cat(paste('Accuracy = ', as.character(accuracy)), '\\n \\n')                           \n",
    "\n",
    "    ## Compute and print precision, recall and F1\n",
    "    precision = sapply(1:nrow(cm), function(i) cm[i,i]/sum(cm[i,]))\n",
    "    recall = sapply(1:nrow(cm), function(i) cm[i,i]/sum(cm[,i]))    \n",
    "    F1 = sapply(1:nrow(cm), function(i) 2*(recall[i] * precision[i])/(recall[i] + precision[i]))    \n",
    "    metrics = sapply(c(precision, recall, F1), round, 3)        \n",
    "    metrics = t(matrix(metrics, nrow = nrow(cm), ncol = ncol(cm)))       \n",
    "    dimnames(metrics) = list(c('Precision', 'Recall', 'F1'), unique(test$Species))      \n",
    "    print(metrics)\n",
    "}  \n",
    "                \n",
    "# use the function print_metrics() on the test data, with 'Species' as the label                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice the following:\n",
    "1. The confusion matrix has dimension 3X3. You can see that most cases are correctly classified with only a few errors. \n",
    "2. The overall accuracy is 0.96. Since the classes are roughly balanced, this metric indicates relatively good performance of the classifier, particularly since it was only trained on 50 cases. \n",
    "3. The precision, recall and  F1 for each of the classes is quite good.\n",
    "|\n",
    "To get a better feel for what the classifier is doing, the code in the cell below displays a set of plots showing correctly (as '+') and incorrectly (as 'o') cases, with the species color-coded. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these plots. You can see how the classifier has divided the feature space between the classes. Notice that most of the errors occur in the overlap region between Virginica and Versicolor. This behavior is to be expected.  \n",
    "\n",
    "Is it possible that a random forest model with more trees would separate these cases better? The code in the cell below will use a model with 100 trees (estimators). This model is fit with the training data and displays the evaluation of the model. \n",
    "\n",
    "Create this code and answer **Question 1** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed to 1115\n",
    "\n",
    "\n",
    "# create a random forest model \"rf_mod\" by training a model with randomForest() to predict \"Species\" of the training data.\n",
    "# use all the available remaining variables as predictors\n",
    "# set the number of trees (ntrees) to 100\n",
    "\n",
    "\n",
    "# use the rf_mod to predict the test data with, using predict(), and using test as newdata. \n",
    "# Assign this prediction to test$scores\n",
    "\n",
    "\n",
    "# use the function print_metrics() on the test data, with 'Species' as the label    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are identical to the model with 5 trees. \n",
    "\n",
    "Like most tree-based models, random forest models have a nice property that **feature importance** is computed during model training. Feature importance can be used as a feature selection method. The `varImp` function from the Caret package performs the calculation.  \n",
    "\n",
    "Execute the code in the cell below to display a plot of the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "imp = varImp(rf_mod)\n",
    "imp[,'Feature'] = row.names(imp)\n",
    "ggplot(imp, aes(x = Feature, y = Overall)) + geom_point(size = 4) +\n",
    "       ggtitle('Variable importance for iris features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the plot displayed above. Notice that the Speal_Lenght and Sepal_Width have rather low importance. \n",
    "\n",
    "Should these features be dropped from the model? To find out, you will create a model with a reduced feature set and compare the results. As a first step, execute the code in the cell below to create training and test datasets using the reduced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed to 1115\n",
    "\n",
    "\n",
    "# create a random forest model \"rf_mod\" by training a model with randomForest() to predict \"Species\" of the training data.\n",
    "# use Petal.Length + Petal.Width as predictors\n",
    "# set the number of trees (ntrees) to 100\n",
    "\n",
    "\n",
    "# use the rf_mod to predict the test data with, using predict(), and using test as newdata. \n",
    "# Assign this prediction to test$scores\n",
    "\n",
    "\n",
    "# use the function print_metrics() on the test data, with 'Species' as the label   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have executed the code, answer **Question 2** on the course page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are a bit better which may indicate the original model was over-fit. Given that a simpler model is more likely to generalize, this model is preferred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Create column of correct-incorrect classification\n",
    "test$correct = ifelse(test$Species == test$scores, 'correct', 'incorrect')\n",
    "\n",
    "single_plot_classes = function(df, colx, coly){\n",
    "    ggplot(df, aes_string(colx,coly)) +\n",
    "          geom_point(aes(color = factor(df$Species), shape = correct), alpha = 0.4)\n",
    "}\n",
    "\n",
    "plot_iris_classes = function(df){\n",
    "    options(repr.plot.width=8, repr.plot.height=5)\n",
    "    grid.arrange(\n",
    "        single_plot_classes(df, 'Sepal.Length', 'Sepal.Width'),\n",
    "        single_plot_classes(df, 'Sepal.Length', 'Petal.Length'),\n",
    "        single_plot_classes(df, 'Petal.Length', 'Petal.Width'),\n",
    "        single_plot_classes(df, 'Sepal.Width', 'Petal.Length'),\n",
    "        nrow = 2)\n",
    "}\n",
    "\n",
    "plot_iris_classes(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "\n",
    "Now, you will try a more complex example using the credit scoring data. You will use the prepared data which has been prepared by removing duplicate cases. Some columns which are know not to be predictive are removed. Finish the code in the cell below to load the dataset for the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the German_Credit_Preped.csv file that you prepped before and name it \"credit\". Set header to TRUE while reading.\n",
    "\n",
    "\n",
    "## Subset the data frame with only the following variables\n",
    "credit = credit[,c('checking_account_status', 'loan_duration_mo', 'credit_history', 'loan_amount', 'savings_account_balance',\n",
    "                   'time_employed_yrs', 'payment_pcnt_income', 'time_in_residence', 'property', 'age_yrs',\n",
    "                   'other_credit_outstanding', 'number_loans', 'job_category', 'dependents', 'telephone', 'bad_credit' )]\n",
    "\n",
    "# inspect the credit dimensions\n",
    "\n",
    "# inspect the credit names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation will be used to train the model. Since folds will be selected from the entire dataset the numeric features are scaled in batch. Execute the code in the cell below to accomplish this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the variable \"num_cols\" and assign 'loan_duration_mo', 'loan_amount', 'payment_pcnt_income', 'time_in_residence', \n",
    "# 'age_yrs', 'number_loans', 'dependents' to it\n",
    "\n",
    "# pre process the numeric variables\n",
    "preProcValues <- preProcess(credit[,num_cols], method = c(\"center\", \"scale\"))\n",
    "\n",
    "# use the predict function and the preProcValues to apply this transformation to the numeric columns of the \n",
    "# credit data\n",
    "\n",
    "\n",
    "# inspect the numeric columns of the credit data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R Caret package computes most performance metrics using the positive cases. For example, recall is a measure of correct classification of positive cases. Therefore, it is important to have the coding of the label correct. The code in the cell below creates a factor (categorical) variable and coerces the levels of the label column, `bad_credit`. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit$bad_credit <- ifelse(credit$bad_credit == 1, 'bad', 'good')\n",
    "credit$bad_credit <- factor(credit$bad_credit, levels = c('bad', 'good'))\n",
    "credit$bad_credit[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the results above you can see the new coding of the label column along with the levels, {'bad', 'good'}. \n",
    "\n",
    "As the inner loop of a nested cross validation, the code in the cell below uses the capability of the R Caret package to estimate the best hyperparameters using 5 fold cross validation. This first cross validation is performed using ROC as the metric. There are a few points to note here:\n",
    "1. A Caret `trainControl` object is used to define the 5 fold cross validation. The `twoClassSummary` function is specified, making ROC the metric for hyperparameter optimization. \n",
    "2. The model is trained using all features as can be seen from the model formula in the Caret `train` function. \n",
    "3. `ROC` is specified as a `metric` in the call to `train`. \n",
    "4. Weights are specified to help with the class imbalance and the cost imbalance of misclassification of bad credit customers. \n",
    "5. The `train` function uses a `tuneGrid` argument to define the hyperparameters to search. \n",
    "\n",
    "Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight vector \"weight\" using ifelse(). If bad_credit is 'bad', assign 0.9, else 0.1\n",
    "\n",
    "\n",
    "fitControl <- trainControl(method = \"cv\",\n",
    "                           number = 5,\n",
    "                           returnResamp=\"all\",\n",
    "                           savePredictions = TRUE,\n",
    "                           classProbs = TRUE,\n",
    "                           summaryFunction = twoClassSummary)\n",
    "\n",
    "paramGrid <- expand.grid(mtry = c(5, 10, 15))\n",
    "\n",
    "# set the seed to 1234\n",
    "\n",
    "# train a model \"rf_fit_inside_tw\" with train()\n",
    "# inspect the train() function with ?train in R studio to better understand the below criteria\n",
    "# use bad_credit as label\n",
    "# use all the other variables as predictors\n",
    "# use credit data to train the model with\n",
    "# set method to \"rf\" (Random Forest)\n",
    "# use fitControl as trControl\n",
    "# use paramGrid as tuneGrid\n",
    "# use weights as weights\n",
    "# set metric to \"ROC\"\n",
    "\n",
    "\n",
    "# print your rf_fit_inside_tw model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid of hyperpameters searched by the Caret package is only `mtry`. The printed tables shows the values of the metrics as a function of the parameters in the search grid. Sens is short for sensitivity which is the same as global recall and Spec is specificity which is the true negative rate $= \\frac{TN}{TN + FP}$\n",
    "\n",
    "The hyperparameter optimization can also be performed using Recall as a metric. The code in the cell below uses the `prSummary` function for the `summaryFunction` argument for `trainControl` and sets the `metric` as `Recall`. \n",
    "\n",
    "Execute this call and examine the results. Then, answer **Question 3** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitControl <- trainControl(method = \"cv\",\n",
    "                           number = 5,\n",
    "                           returnResamp=\"all\",\n",
    "                           savePredictions = TRUE,\n",
    "                           classProbs = TRUE,\n",
    "                           summaryFunction = prSummary)\n",
    "\n",
    "paramGrid <- expand.grid(mtry = c(5, 15, 25))\n",
    "\n",
    "# set the seed to 1234\n",
    "\n",
    "# train a model \"rf_fit_inside_pr\" with train()\n",
    "# inspect the train() function with ?train in R studio to better understand the below criteria\n",
    "# use bad_credit as label\n",
    "# use all the other variables as predictors\n",
    "# use credit data to train the model with\n",
    "# set method to \"rf\" (Random Forest)\n",
    "# use fitControl as trControl\n",
    "# use paramGrid as tuneGrid\n",
    "# use weights as weights\n",
    "# set metric to \"Recall\"\n",
    "\n",
    "\n",
    "# print your rf_fit_inside_pr model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the problem, these results seem reasonable. \n",
    "\n",
    "The question now is, given the optimal hyperparameters, which features are the most important? The code in the cell below computes and displays feature importance using the Caret `varImp` function. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=6)\n",
    "var_imp = varImp(rf_fit_inside_pr)\n",
    "print(var_imp)\n",
    "plot(var_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In is clear that some of the features are not important to model performance. Execute the code in the cell below to prune the feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_reduced = credit[,c('loan_amount', 'age_yrs', 'loan_duration_mo', 'checking_account_status', 'time_in_residence',\n",
    "                          'payment_pcnt_income', 'savings_account_balance', 'other_credit_outstanding', \n",
    "                          'credit_history', 'time_employed_yrs', 'property', 'number_loans', 'telephone',\n",
    "                          'bad_credit')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now run the inside loop of the CV to test the reduced feature set. Execute the code in the cell below to perform the cross validation grid search using the reduced feature set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed to 3344\n",
    "\n",
    "# train a model \"rf_fit_inside_pr\" with train()\n",
    "# inspect the train() function with ?train in R studio to better understand the below criteria\n",
    "# use bad_credit as label\n",
    "# use all the other variables as predictors\n",
    "# use credit_reduced data to train the model with\n",
    "# set method to \"rf\" (Random Forest)\n",
    "# use fitControl as trControl\n",
    "# use paramGrid as tuneGrid\n",
    "# use weights as weights\n",
    "# set metric to \"Recall\"\n",
    "\n",
    "\n",
    "# print your rf_fit_inside_pr model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the cross validation grid search with the reduced feature are slightly lower than before. However, this difference is unlikely to be significant. The optimal value of `mtry` is still 25. Evidentially, pruning these features was the correct step. This process can be continued, but will not be in this lab in the interest of reducing length. \n",
    "\n",
    "To verify that the model will generalize well it is time to perform the outside CV loop. The code in the cell below defines a parameter grid with just the optimal hyperparameter value. The CV then repeatedly fits the model with this single hyperparameter. \n",
    "\n",
    "Execute this code, examine the result, and answer **Question 4** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the hyperparameter grid to the optimal values from the inside loop\n",
    "paramGrid <- expand.grid(mtry = c(rf_fit_inside_pr$finalModel$mtry))\n",
    "\n",
    "\n",
    "# set the seed to 5678\n",
    "\n",
    "# train a model \"rf_fit_outside_pr\" with train()\n",
    "# inspect the train() function with ?train in R studio to better understand the below criteria\n",
    "# use bad_credit as label\n",
    "# use all the other variables as predictors\n",
    "# use credit_reduced data to train the model with\n",
    "# set method to \"rf\" (Random Forest)\n",
    "# use fitControl as trControl\n",
    "# use paramGrid as tuneGrid\n",
    "# use weights as weights\n",
    "# set metric to \"Recall\"\n",
    "\n",
    "\n",
    "print_metrics = function(mod){\n",
    "    means = c(apply(mod$resample[,1:4], 2, mean), mtry = mod$resample[1,5], Resample = 'Mean')\n",
    "    stds = c(apply(mod$resample[,1:4], 2, sd), mtry = mod$resample[1,5], Resample = 'STD')\n",
    "    out = rbind(mod$resample, means, stds)\n",
    "    out[,1:4] = lapply(out[,1:4], function(x) round(as.numeric(x), 3))\n",
    "    out\n",
    "}\n",
    "                       \n",
    "# use the function print_metrics() to print your rf_fit_outside_pr model\n",
    "print_metrics(rf_fit_outside_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice that the standard deviation of the mean of the AUC are nearly an order of magnitude smaller than the mean. This indicates that this model is likely to generalize well. \n",
    "\n",
    "***\n",
    "**Note:** The predict method can be used with this optimal model to classify unknown cases.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have accomplished the following:\n",
    "1. Used a random forest model to classify the cases of the iris data. A model with more trees had marginally lower error rates, but likely not significantly different.\n",
    "2. Used 5 fold to find estimated optimal hyperparameters for a random forest model to classify credit risk cases. This process is the inner loop of a nested cross validation process. \n",
    "3. Applied feature importance was used for feature selection with the iris data. The model created and evaluated with the reduced feature set had essentially the same performance as the model with more features.  \n",
    "4. Performed the outer loop of nested cross validation with a 5 fold CV to confirm that the model will likely generalize well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
